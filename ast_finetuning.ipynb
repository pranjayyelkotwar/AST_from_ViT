{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Speech Commands Classification using Audio Spectrogram Transformer (AST)\n",
    "\n",
    "This script loads the Speech Commands dataset, preprocesses audio data into Mel spectrograms, \n",
    "and fine-tunes a Vision Transformer (ViT) model, adapting it to work as an AST for audio classification.\n",
    "The model is trained using a supervised learning approach, with early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "Dependencies:\n",
    "- PyTorch\n",
    "- TorchAudio\n",
    "- timm (for pretrained ViT models)\n",
    "- Librosa (for audio processing)\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "import torchaudio.transforms as transforms\n",
    "\n",
    "# Ensure the dataset directory exists\n",
    "os.makedirs(\"./SpeechCommands\", exist_ok=True)\n",
    "\n",
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    \"\"\"\n",
    "    Custom subset of the Speech Commands dataset, allowing filtering by training, validation, or testing sets.\n",
    "    \"\"\"\n",
    "    def __init__(self, root: str = \"./SpeechCommands\", subset: str = None):\n",
    "        super().__init__(root=root, download=True)\n",
    "        \n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as f:\n",
    "                return [os.path.join(self._path, line.strip()) for line in f]\n",
    "        \n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = set(load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\"))\n",
    "            self._walker = [w for w in self._walker if w not in excludes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function to handle varying waveform lengths\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads waveforms in a batch to the maximum length found within the batch.\n",
    "    \"\"\"\n",
    "    waveforms, sample_rates, labels, speaker_ids, utterance_numbers = zip(*batch)\n",
    "    max_length = max(waveform.shape[1] for waveform in waveforms)\n",
    "    padded_waveforms = [torch.nn.functional.pad(w, (0, max_length - w.shape[1])) for w in waveforms]\n",
    "    return torch.stack(padded_waveforms), torch.tensor(sample_rates), labels, speaker_ids, utterance_numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset objects\n",
    "train_set = SubsetSC(root=\"./SpeechCommands\", subset=\"training\")\n",
    "val_set   = SubsetSC(root=\"./SpeechCommands\", subset=\"validation\")\n",
    "test_set  = SubsetSC(root=\"./SpeechCommands\", subset=\"testing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data loading\n",
    "for batch in train_loader:\n",
    "    waveforms, sample_rates, labels, speaker_ids, utterance_numbers = batch\n",
    "    print(\"Waveforms shape:\", waveforms.shape)  # Should be [batch_size, 1, max_length]\n",
    "    print(\"Sample rate:\", sample_rates[0])\n",
    "    print(\"Labels:\", labels)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup: Vision Transformer (ViT) Adapted for Audio Spectrogram Transformer (AST)\n",
    "\n",
    "We use a ViT model as the backbone, modifying it to accept single-channel spectrogram inputs\n",
    "and adapting its classifier head for speech command recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load a pretrained Vision Transformer model\n",
    "model = timm.create_model('vit_base_patch16_224_in21k', pretrained=True)\n",
    "model.patch_embed.img_size = (128, 128)  # Adjust expected input size\n",
    "model.default_cfg['img_size'] = 128\n",
    "\n",
    "# Load pretrained AST weights\n",
    "ast_pretrained_weight = torch.load(\"audioset_16_16_0.4422.pth\")\n",
    "\n",
    "# Modify the model for AST input\n",
    "model.patch_embed.proj = nn.Conv2d(1, 768, kernel_size=(8, 8), stride=(8, 8))\n",
    "num_tokens_ast = 257  # 256 patches + 1 CLS token\n",
    "model.pos_embed = nn.Parameter(torch.randn(1, num_tokens_ast, 768) * .02)\n",
    "\n",
    "# Transfer weights from AST to ViT\n",
    "v = model.state_dict()\n",
    "v['cls_token'] = ast_pretrained_weight['module.v.cls_token']\n",
    "for i in range(12):  # Transfer transformer block weights\n",
    "    for key in ['norm1', 'attn.qkv', 'attn.proj', 'norm2', 'mlp.fc1', 'mlp.fc2']:\n",
    "        v[f'blocks.{i}.{key}.weight'] = ast_pretrained_weight[f'module.v.blocks.{i}.{key}.weight']\n",
    "        v[f'blocks.{i}.{key}.bias'] = ast_pretrained_weight[f'module.v.blocks.{i}.{key}.bias']\n",
    "v['norm.weight'] = ast_pretrained_weight['module.v.norm.weight']\n",
    "v['norm.bias'] = ast_pretrained_weight['module.v.norm.bias']\n",
    "model.load_state_dict(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify classifier head for speech commands\n",
    "num_speech_commands = 35\n",
    "model.head = nn.Linear(model.head.in_features, num_speech_commands)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping setup\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with tqdm for progress tracking\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "csv_filename = \"training_metrics.csv\"\n",
    "with open(csv_filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Epoch\", \"Train Loss\", \"Train Accuracy\", \"Val Loss\", \"Val Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for spectrograms, labels in tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Training\", leave=False):\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(spectrograms)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item() * spectrograms.size(0)\n",
    "    train_loss, train_accuracy = running_loss / len(train_dataset), 100 * correct / total\n",
    "    val_loss, val_accuracy = evaluate_model(model, val_loader)\n",
    "    \n",
    "    with open(csv_filename, mode='a', newline='') as file:\n",
    "        writer.writerow([epoch + 1, train_loss, train_accuracy, val_loss, val_accuracy])\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss, epochs_without_improvement = val_loss, 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            break\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
